{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 6: Deep Learning and the Higgs Dataset\n",
    "\n",
    "\n",
    "#### Steven Hayden, Kevin Mendonsa, Joe Schueder, Nicole Wittlin  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of *high energy physics*, observing exotic particles and measuring their properties may yield critical insights about the nature of matter. However, the data for analyzing these exotic particles is extremely sparse. For example, the Large Hadron Collider produces approximately 10^11 collisions per hour, yet only roughly 300 of these collisions result in the desired exotic particles. Therefore, good data analysis depends on distinguishing collisions which produce particles of interest (signal) from those producing other particles (background).\n",
    "\n",
    "Machine learning, and specifically signal-versus-background classification, can aid in the analysis process. In 2014, Baldi et al. published the results of their work in \"Searching for Exotic Particles in High-Energy Phyics with Deep Learning\" to highlight how neural networks and deep learning approaches can improve the collider searches for exotic particles. \n",
    "\n",
    "Our objective is to recreate the neural network specifications used by Baldi and team leveraging updated techiques, as well as suggest improvements on their existing work given the advances in deep learning in the past six years.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WL8UoOTmGGsL"
   },
   "source": [
    "## Data Preparation and Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baldi and team published a dataset of 11 million simulated collision events for benchmarking machine learning classification algorithms in distinguishing exotic particles (known as Higgs bosons). This data was produced using a Monte Carlo simulation and has 28 features: the first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator and the last seven features are high-level features derived by physicists from the first 21 features to help discriminate between the two classes. This dataset is known as the Higgs data.\n",
    "\n",
    "The data set used in the Baldi analysis was nearly balanced with 53% positive examples in Higgs data. \n",
    "\n",
    "We did not conduct true data exploration on the data set; instead, we focused on confirming we had the correct information in a usable format. We opted to import the data as a dataframe, rather than a tensor. This choice made the data more readable and allowed for more straightforward analysis. We acknowledge that this may have a negative impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FklhSI0Gg9R"
   },
   "source": [
    "### Package Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pZ8A2liqvgk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pnOU-ctX27Q"
   },
   "outputs": [],
   "source": [
    "from  IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jj6I4dvTtbUe"
   },
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "proxy = ''\n",
    "os.environ['http_proxy'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dataset = '/home/jjschued/HIGGS.csv'\n",
    "#df = pd.read_csv('C:/Users/shayden/Downloads/HIGGS.csv.gz', compression = 'gzip',nrows=11000, header = None)\n",
    "#url_dataset = 'C:/Users/shayden/Downloads/HIGGS.csv'\n",
    "#df = pd.read_csv(url_dataset, nrows=1000000)\n",
    "df = pd.read_csv(url_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jjschued/HIGGS.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10999999 entries, 0 to 10999998\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Dtype  \n",
      "---  ------                      -----  \n",
      " 0   1.000000000000000000e+00    float64\n",
      " 1   8.692932128906250000e-01    float64\n",
      " 2   -6.350818276405334473e-01   float64\n",
      " 3   2.256902605295181274e-01    float64\n",
      " 4   3.274700641632080078e-01    float64\n",
      " 5   -6.899932026863098145e-01   float64\n",
      " 6   7.542022466659545898e-01    float64\n",
      " 7   -2.485731393098831177e-01   float64\n",
      " 8   -1.092063903808593750e+00   float64\n",
      " 9   0.000000000000000000e+00    float64\n",
      " 10  1.374992132186889648e+00    float64\n",
      " 11  -6.536741852760314941e-01   float64\n",
      " 12  9.303491115570068359e-01    float64\n",
      " 13  1.107436060905456543e+00    float64\n",
      " 14  1.138904333114624023e+00    float64\n",
      " 15  -1.578198313713073730e+00   float64\n",
      " 16  -1.046985387802124023e+00   float64\n",
      " 17  0.000000000000000000e+00.1  float64\n",
      " 18  6.579295396804809570e-01    float64\n",
      " 19  -1.045456994324922562e-02   float64\n",
      " 20  -4.576716944575309753e-02   float64\n",
      " 21  3.101961374282836914e+00    float64\n",
      " 22  1.353760004043579102e+00    float64\n",
      " 23  9.795631170272827148e-01    float64\n",
      " 24  9.780761599540710449e-01    float64\n",
      " 25  9.200048446655273438e-01    float64\n",
      " 26  7.216574549674987793e-01    float64\n",
      " 27  9.887509346008300781e-01    float64\n",
      " 28  8.766783475875854492e-01    float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.4 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.rename(columns={x:y for x,y in zip(df.columns,range(0,len(df.columns)))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating the Original Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baldi and team chose optimal hyper-parameters for the deep learning neural network using a subset of the Higgs data consisting of 2.6M training examples and 100,000 validation examples. They note that the computational costs of the process did not allow for thorough optimization; however they combined pre-training methods, network architectures, initial learning rates, and regularization methods to determine the best criteria. \n",
    "\n",
    "The hyper-parameter optimization was performed using the full set of Higgs features, and the various classifiers algorithms.  These classifiers were tested on 500,000 simulated examples generated using Monte Carlo procedures as training sets. The best performing model, as determined by the Receiver Operating Characteristic (ROC) curves is outlined below. \n",
    "\n",
    "**Optimal Model from Baldi and team**\n",
    "- Five-layer Neural Network with 300 hidden units in each layer\n",
    "- Learning Rate of 0.05\n",
    "- Weight Decay Coefficient 1 x 10-5\n",
    "- Predetermined without Optimization  \n",
    "    - Hidden units all used the **tahn** activation function\n",
    "    - Weights initialized from normal distribution with 0 mean and standard deviation 0.1 in first layer, 0.001 in output layer, and 0.05in all other hidden layers\n",
    "    - Mini-batches of size 100 to compute gradients\n",
    "    - Momentum increased linearly over the first 200 epochs from 0.9 to 0.99, then remained constant\n",
    "    - Learning Rate decayed by 1.0000002 every batch update until it reached a minimum of 10-6\n",
    "- Training ended when momentum reached maximum value and minimum error on validation set of 500,000 examples had not decreased by more than a factor of 0.00001 over 10 epochs. Early stopping prevented overfitting and each NN trained over 200-1000 epochs.\n",
    "- When training with dropout, increased learning rate decay to 1.0000003 and ended training when the momentum reached maximum value and error on validation set had not decreased for 40 epochs\n",
    "- Inputs standardized over entire train/test set with mean 0 and standard deviation of 1, except for features with values strictly greater than 0 (scaled for mean value of 1)\n",
    "- An additional boost in performance is obtained by using the dropout training algorithm, in which we stochastically drop neurons in the top hidden layer with 50% probability during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmk49OqZIFZP"
   },
   "outputs": [],
   "source": [
    "N_VALIDATION = int(1e3)\n",
    "N_TRAIN = int(1e4)\n",
    "BUFFER_SIZE = int(1e4)\n",
    "BATCH_SIZE = 100\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def schedule( lr):\n",
    "#   if lr > 0.000001:\n",
    "#       return float(lr)\n",
    "#   else:\n",
    "#       return float(0.000001)\n",
    "\n",
    "    \n",
    "# learning rate schedule\n",
    "def schedule(epoch,lr):\n",
    "    initial_learning_rate=0.05\n",
    "    decay = 0.0000002\n",
    "    decay_steps= float(STEPS_PER_EPOCH*1000)\n",
    "    if lr > 0.000001:\n",
    "        LearningRate = initial_learning_rate / (1 + decay * epoch/ decay_steps)  \n",
    "        print(epoch)              \n",
    "        return float(LearningRate)\n",
    "    else:\n",
    "        LearningRate=float(0.000001)\n",
    "        print(epoch)\n",
    "        return LearningRate\n",
    "    \n",
    "   \n",
    "    \n",
    "def get_callbacks():\n",
    "    \n",
    "  return [\n",
    "#    tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', min_delta = 0.00001, mode = 'min', patience=10),\n",
    "    tf.keras.callbacks.LearningRateScheduler(schedule)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer():\n",
    "    \n",
    "  return tf.keras.optimizers.SGD(learning_rate=0.05,momentum=0.90, decay = 0.0000002, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parms needed for case study  \n",
    "\n",
    "#We selected a five-layer neural network with 300 hidden units in each layer,\n",
    "#a learning rate of 0.05, and a weight decay coefficient of 1 × 10−5.\n",
    "# Hidden layer have tanh activation function\n",
    "#Gradient computations were made on mini-batches of size 100\n",
    "#The learning rate decayed by a factor of 1.0000002 every batch update until it reached a minimum of 10^−6,\n",
    "\n",
    "def define_predictor():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(300, kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None), input_shape= X_train.shape[1:], activation='tanh')) # Hidden 1\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 2\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 3\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 4\n",
    "    model.add(tf.keras.layers.Dense(300,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation='tanh')) # Hidden 5\n",
    "    model.add(tf.keras.layers.Dense(1,kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001, seed=None),activation='sigmoid')) # Output #1.2\n",
    "    #model.add(tf.keras.layers.Dense(1,activation='softmax')) # Output\n",
    "    #sgd = tf.keras.optimizers.SGD(lr=.05, decay = 1.0000002, momentum=0.99)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  optimizer=get_optimizer() , \n",
    "                  metrics=['accuracy', \n",
    "                            tf.keras.losses.BinaryCrossentropy(\n",
    "                            from_logits=True, name='val_binary_crossentropy'), \n",
    "                            tf.keras.metrics.AUC()])\n",
    "    return model\n",
    "\n",
    "    \n",
    "#setup scaler\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "dfx\n",
    "X = dfx.loc[:, 1:28]\n",
    "Y = dfx[0].copy()\n",
    "y = Y.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, TimeSeriesSplit, StratifiedShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X_test, y_test, test_size = 0.5, random_state = 101)\n",
    "#N_VALIDATION = int(1e3)\n",
    "\n",
    "BATCH_SIZE = 3000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden layers** - help models extrapolate relationships between variables. One hidden layer is enough to estimate first order and second order interactions. The larger the number of hidden layers in a neural network, the longer it will take for the neural network to produce the output and the more complex problems the neural network can solve. Due to the complexity of our problem additional hidden layers were needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epoch** – represents the number of completes passes through the training dataset during the learning process, where the learning algorithm loops through a fixed number of epochs and within each, updates the network for each row in the training data; one epoch means that each sample in the training dataset has updated internal parameters; calculated as N / batch size training iterations, where N is the total number of examples. \n",
    "\n",
    "*“You can think of a for-loop over the number of epochs where each loop proceeds over the training dataset. Within this for-loop is another nested for-loop that iterates over each batch of samples, where one batch has the specified “batch size” number of samples.”* - Jason Brownlee, PhD, https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iteration** – number of batches needed to complete on epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Iterations, Batches, and Epochs**\n",
    "\n",
    "Dataset with 200 samples<br>\n",
    "Batch size = 5<br>\n",
    "Epochs = 1000\n",
    "- Dataset will be divided into 40 batches, each with 5 samples, model weights will update after each batch of 5 samples\n",
    "- One epoch will involve 40 batches/40 updates to model\n",
    "- 1000 epochs, model will be exposed/passed through whole data 1000 times, total of 40,000 batches during entire training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch/batch size** – the number of data points/observations used in one iteration (one gradient update) of model training and dictates the number of training observations to be “learned” before updating internal parameters; generally, a larger batch involves more training examples, thus yielding a more stable learning process and accurate estimate. For example, a batch size of 32 means that 32 samples from the training dataset will be used to estimate error gradient before the model weights are updated. *NOTE: batch size and number of batches are different.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate** – a configurable hyper-parameter that controls how quickly/slowly a neural network learns a problem, more specifically it controls how much to change the weights to correct for error during each iteration; a large learning rate allows model to train faster but a cost, where smaller learning rates may yield a better model, requiring more training epochs and smaller batch sizes. \n",
    "- Math: gradient descent algorithm multiples learning rate by gradient, for example value of 0.1 will update weight 10% of the amount it could be updated\n",
    "- Range: 0.0 to 1.0, traditional default 0.1 or 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate Decay** – how learning rate changes over training epochs; learning rate decay can be designed where large weight changes happen at the beginning of the process and smaller, fine-tune changes toward the end; another strategy is to decay over a fixed number of training epochs at a small, constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Out** – form of regularization to minimize overfitting; technique that randomly removes/inactivates neurons at each training step, which forces remaining neurons to be more independent because they learn rated not in conjunction/cooperation with neighboring neuron; roughly doubles the number of iterations required for convergence but training time for each epoch is less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Functions** – aka transfer functions; functions that take a weighted sum of all inputs from previous layer and generates an output value for the next layer; for each node, it defines the output of the node given an input or set of inputs. \n",
    "- Tanh activation – a non-linear activation function that outputs values between -1.0 and 1.0 and the center falls around 0; limitations are that it can have limited sensitivity and is prone to saturation in larger, more layered networks due to vanishing gradients.\n",
    "- Sigmoid activation – a non-linear activation function similar to a logistic function, where inputs produce a result between 0 and 1; sigmoid outputs are not centered around 0; not necessarily good for many layered networks due to vanishing gradients, and drawbacks include it can be prone to saturation and kill gradient movement.\n",
    "![title](img/tanh.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent** - A method for looking for minimum or maximum values for a parameter.  The data is evaluatied in computations to determine if the current comutation is moving towards the goal minimum or maximum compared to the last computation.   \n",
    "\n",
    "**Types of Gradient Descent** \n",
    "- **Batch Gradient Descent** – batch size is set to total number of examples in the training dataset\n",
    "- **Stochastic Gradient Descent** – batch size is set to one\n",
    "- **Minibatch Gradient Descent** – batch size is set to more than one and less than the total number of examples in the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent Parameters**\n",
    " - **Nesterov** - In the standard Momentum method, the gradient is computed using current parameters. Nesterov momentum achieves stronger convergence by applying the velocity to the parameters in order to compute interim parameters. These interim parameters are then used to compute the gradient, called a \"lookahead\" gradient step. (https://golden.com/wiki/Nesterov_momentum ) \n",
    " - **Momentum** - Determines how much previous gradients effect the current iteration. Ususally this is set to .9 which allows the model to make large leaps when moving in one direction, but decreases when the most recent result finds a change in direction. It im improves the speed of optimization in concert with step size by helping SGD algorithm navigate in relevant/optimal directions; in other words, it adds inertia to the algorithm update process to continue moving in the optimal direction; best to begin with smaller momentum and then increase after passing through larger gradients – momentum can cause learning process to miss or oscillate around the minima. \n",
    "- Math: adds a fraction of the direction of the previous step to a current step\n",
    "- Range: 0.0 to 1.0, traditional default 0.9, 0.99 or 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/50\n",
      "2567/2567 - 22s - loss: 0.5829 - accuracy: 0.6859 - val_binary_crossentropy: 0.6689 - auc_8: 0.7548 - val_loss: 0.6713 - val_accuracy: 0.5858 - val_val_binary_crossentropy: 0.6929 - val_auc_8: 0.6267\n",
      "1\n",
      "Epoch 2/50\n",
      "2567/2567 - 21s - loss: 0.5260 - accuracy: 0.7324 - val_binary_crossentropy: 0.6479 - auc_8: 0.8117 - val_loss: 0.6641 - val_accuracy: 0.6183 - val_val_binary_crossentropy: 0.6956 - val_auc_8: 0.6373\n",
      "2\n",
      "Epoch 3/50\n",
      "2567/2567 - 21s - loss: 0.5142 - accuracy: 0.7402 - val_binary_crossentropy: 0.6439 - auc_8: 0.8213 - val_loss: 0.6807 - val_accuracy: 0.6172 - val_val_binary_crossentropy: 0.6988 - val_auc_8: 0.6280\n",
      "3\n",
      "Epoch 4/50\n",
      "2567/2567 - 21s - loss: 0.5085 - accuracy: 0.7442 - val_binary_crossentropy: 0.6419 - auc_8: 0.8259 - val_loss: 0.7055 - val_accuracy: 0.6017 - val_val_binary_crossentropy: 0.7063 - val_auc_8: 0.6238\n",
      "4\n",
      "Epoch 5/50\n",
      "2567/2567 - 21s - loss: 0.5049 - accuracy: 0.7467 - val_binary_crossentropy: 0.6407 - auc_8: 0.8287 - val_loss: 0.6816 - val_accuracy: 0.6112 - val_val_binary_crossentropy: 0.7011 - val_auc_8: 0.6238\n",
      "5\n",
      "Epoch 6/50\n",
      "2567/2567 - 21s - loss: 0.5023 - accuracy: 0.7487 - val_binary_crossentropy: 0.6398 - auc_8: 0.8308 - val_loss: 0.6849 - val_accuracy: 0.6097 - val_val_binary_crossentropy: 0.7024 - val_auc_8: 0.6203\n",
      "6\n",
      "Epoch 7/50\n",
      "2567/2567 - 21s - loss: 0.5002 - accuracy: 0.7499 - val_binary_crossentropy: 0.6391 - auc_8: 0.8323 - val_loss: 0.6794 - val_accuracy: 0.6114 - val_val_binary_crossentropy: 0.6996 - val_auc_8: 0.6264\n",
      "7\n",
      "Epoch 8/50\n",
      "2567/2567 - 21s - loss: 0.4985 - accuracy: 0.7510 - val_binary_crossentropy: 0.6385 - auc_8: 0.8337 - val_loss: 0.6994 - val_accuracy: 0.6010 - val_val_binary_crossentropy: 0.7055 - val_auc_8: 0.6240\n",
      "8\n",
      "Epoch 9/50\n",
      "2567/2567 - 21s - loss: 0.4971 - accuracy: 0.7520 - val_binary_crossentropy: 0.6380 - auc_8: 0.8347 - val_loss: 0.7138 - val_accuracy: 0.5974 - val_val_binary_crossentropy: 0.7082 - val_auc_8: 0.6145\n",
      "9\n",
      "Epoch 10/50\n",
      "2567/2567 - 21s - loss: 0.4959 - accuracy: 0.7528 - val_binary_crossentropy: 0.6376 - auc_8: 0.8356 - val_loss: 0.7128 - val_accuracy: 0.5964 - val_val_binary_crossentropy: 0.7090 - val_auc_8: 0.6250\n",
      "10\n",
      "Epoch 11/50\n",
      "2567/2567 - 21s - loss: 0.4948 - accuracy: 0.7534 - val_binary_crossentropy: 0.6372 - auc_8: 0.8365 - val_loss: 0.7213 - val_accuracy: 0.5965 - val_val_binary_crossentropy: 0.7094 - val_auc_8: 0.6167\n",
      "11\n",
      "Epoch 12/50\n",
      "2567/2567 - 21s - loss: 0.4938 - accuracy: 0.7542 - val_binary_crossentropy: 0.6369 - auc_8: 0.8372 - val_loss: 0.7288 - val_accuracy: 0.5941 - val_val_binary_crossentropy: 0.7110 - val_auc_8: 0.6176\n",
      "12\n",
      "Epoch 13/50\n",
      "2567/2567 - 21s - loss: 0.4930 - accuracy: 0.7547 - val_binary_crossentropy: 0.6366 - auc_8: 0.8378 - val_loss: 0.7235 - val_accuracy: 0.5951 - val_val_binary_crossentropy: 0.7099 - val_auc_8: 0.6208\n",
      "13\n",
      "Epoch 14/50\n",
      "2567/2567 - 21s - loss: 0.4919 - accuracy: 0.7555 - val_binary_crossentropy: 0.6362 - auc_8: 0.8387 - val_loss: 0.7454 - val_accuracy: 0.5843 - val_val_binary_crossentropy: 0.7151 - val_auc_8: 0.6144\n",
      "14\n",
      "Epoch 15/50\n",
      "2567/2567 - 21s - loss: 0.4912 - accuracy: 0.7558 - val_binary_crossentropy: 0.6360 - auc_8: 0.8392 - val_loss: 0.7685 - val_accuracy: 0.5818 - val_val_binary_crossentropy: 0.7181 - val_auc_8: 0.6146\n",
      "15\n",
      "Epoch 16/50\n",
      "2567/2567 - 21s - loss: 0.4904 - accuracy: 0.7564 - val_binary_crossentropy: 0.6358 - auc_8: 0.8397 - val_loss: 0.7396 - val_accuracy: 0.5879 - val_val_binary_crossentropy: 0.7128 - val_auc_8: 0.6062\n",
      "16\n",
      "Epoch 17/50\n",
      "2567/2567 - 21s - loss: 0.4896 - accuracy: 0.7570 - val_binary_crossentropy: 0.6355 - auc_8: 0.8404 - val_loss: 0.7356 - val_accuracy: 0.5866 - val_val_binary_crossentropy: 0.7130 - val_auc_8: 0.6085\n",
      "17\n",
      "Epoch 18/50\n",
      "2567/2567 - 21s - loss: 0.4890 - accuracy: 0.7573 - val_binary_crossentropy: 0.6353 - auc_8: 0.8408 - val_loss: 0.7323 - val_accuracy: 0.5897 - val_val_binary_crossentropy: 0.7106 - val_auc_8: 0.6125\n",
      "18\n",
      "Epoch 19/50\n",
      "2567/2567 - 21s - loss: 0.4882 - accuracy: 0.7579 - val_binary_crossentropy: 0.6350 - auc_8: 0.8414 - val_loss: 0.7377 - val_accuracy: 0.5881 - val_val_binary_crossentropy: 0.7125 - val_auc_8: 0.6072\n",
      "19\n",
      "Epoch 20/50\n",
      "2567/2567 - 21s - loss: 0.4878 - accuracy: 0.7581 - val_binary_crossentropy: 0.6349 - auc_8: 0.8417 - val_loss: 0.8088 - val_accuracy: 0.5715 - val_val_binary_crossentropy: 0.7237 - val_auc_8: 0.6018\n",
      "20\n",
      "Epoch 21/50\n",
      "2567/2567 - 21s - loss: 0.4873 - accuracy: 0.7584 - val_binary_crossentropy: 0.6347 - auc_8: 0.8421 - val_loss: 0.7096 - val_accuracy: 0.5920 - val_val_binary_crossentropy: 0.7075 - val_auc_8: 0.6100\n",
      "21\n",
      "Epoch 22/50\n",
      "2567/2567 - 21s - loss: 0.4866 - accuracy: 0.7590 - val_binary_crossentropy: 0.6345 - auc_8: 0.8426 - val_loss: 0.7760 - val_accuracy: 0.5760 - val_val_binary_crossentropy: 0.7196 - val_auc_8: 0.6072\n",
      "22\n",
      "Epoch 23/50\n",
      "2567/2567 - 21s - loss: 0.4862 - accuracy: 0.7591 - val_binary_crossentropy: 0.6343 - auc_8: 0.8429 - val_loss: 0.7480 - val_accuracy: 0.5801 - val_val_binary_crossentropy: 0.7148 - val_auc_8: 0.6005\n",
      "23\n",
      "Epoch 24/50\n",
      "2567/2567 - 21s - loss: 0.4855 - accuracy: 0.7595 - val_binary_crossentropy: 0.6341 - auc_8: 0.8433 - val_loss: 0.7538 - val_accuracy: 0.5797 - val_val_binary_crossentropy: 0.7156 - val_auc_8: 0.6007\n",
      "24\n",
      "Epoch 25/50\n",
      "2567/2567 - 21s - loss: 0.4851 - accuracy: 0.7598 - val_binary_crossentropy: 0.6340 - auc_8: 0.8437 - val_loss: 0.7363 - val_accuracy: 0.5914 - val_val_binary_crossentropy: 0.7104 - val_auc_8: 0.6102\n",
      "25\n",
      "Epoch 26/50\n",
      "2567/2567 - 21s - loss: 0.4847 - accuracy: 0.7603 - val_binary_crossentropy: 0.6338 - auc_8: 0.8440 - val_loss: 0.7289 - val_accuracy: 0.5854 - val_val_binary_crossentropy: 0.7102 - val_auc_8: 0.6039\n",
      "26\n",
      "Epoch 27/50\n",
      "2567/2567 - 21s - loss: 0.4842 - accuracy: 0.7604 - val_binary_crossentropy: 0.6337 - auc_8: 0.8444 - val_loss: 0.7779 - val_accuracy: 0.5746 - val_val_binary_crossentropy: 0.7179 - val_auc_8: 0.5954\n",
      "27\n",
      "Epoch 28/50\n",
      "2567/2567 - 21s - loss: 0.4839 - accuracy: 0.7607 - val_binary_crossentropy: 0.6335 - auc_8: 0.8446 - val_loss: 0.7314 - val_accuracy: 0.5814 - val_val_binary_crossentropy: 0.7101 - val_auc_8: 0.5979\n",
      "28\n",
      "Epoch 29/50\n",
      "2567/2567 - 21s - loss: 0.4835 - accuracy: 0.7609 - val_binary_crossentropy: 0.6334 - auc_8: 0.8448 - val_loss: 0.7449 - val_accuracy: 0.5777 - val_val_binary_crossentropy: 0.7129 - val_auc_8: 0.5939\n",
      "29\n",
      "Epoch 30/50\n",
      "2567/2567 - 21s - loss: 0.4831 - accuracy: 0.7613 - val_binary_crossentropy: 0.6333 - auc_8: 0.8452 - val_loss: 0.7523 - val_accuracy: 0.5781 - val_val_binary_crossentropy: 0.7165 - val_auc_8: 0.5947\n",
      "30\n",
      "Epoch 31/50\n",
      "2567/2567 - 21s - loss: 0.4826 - accuracy: 0.7616 - val_binary_crossentropy: 0.6331 - auc_8: 0.8455 - val_loss: 0.7442 - val_accuracy: 0.5782 - val_val_binary_crossentropy: 0.7145 - val_auc_8: 0.5979\n",
      "31\n",
      "Epoch 32/50\n",
      "2567/2567 - 21s - loss: 0.4824 - accuracy: 0.7617 - val_binary_crossentropy: 0.6330 - auc_8: 0.8457 - val_loss: 0.7483 - val_accuracy: 0.5782 - val_val_binary_crossentropy: 0.7142 - val_auc_8: 0.5925\n",
      "32\n",
      "Epoch 33/50\n",
      "2567/2567 - 21s - loss: 0.4820 - accuracy: 0.7620 - val_binary_crossentropy: 0.6329 - auc_8: 0.8460 - val_loss: 0.7764 - val_accuracy: 0.5710 - val_val_binary_crossentropy: 0.7196 - val_auc_8: 0.5900\n",
      "33\n",
      "Epoch 34/50\n",
      "2567/2567 - 21s - loss: 0.4817 - accuracy: 0.7621 - val_binary_crossentropy: 0.6328 - auc_8: 0.8462 - val_loss: 0.7390 - val_accuracy: 0.5778 - val_val_binary_crossentropy: 0.7122 - val_auc_8: 0.5929\n",
      "34\n",
      "Epoch 35/50\n",
      "2567/2567 - 21s - loss: 0.4813 - accuracy: 0.7625 - val_binary_crossentropy: 0.6327 - auc_8: 0.8465 - val_loss: 0.7974 - val_accuracy: 0.5654 - val_val_binary_crossentropy: 0.7247 - val_auc_8: 0.5936\n",
      "35\n",
      "Epoch 36/50\n",
      "2567/2567 - 21s - loss: 0.4810 - accuracy: 0.7626 - val_binary_crossentropy: 0.6326 - auc_8: 0.8467 - val_loss: 0.7684 - val_accuracy: 0.5725 - val_val_binary_crossentropy: 0.7187 - val_auc_8: 0.5874\n",
      "36\n",
      "Epoch 37/50\n",
      "2567/2567 - 21s - loss: 0.4807 - accuracy: 0.7628 - val_binary_crossentropy: 0.6325 - auc_8: 0.8469 - val_loss: 0.7516 - val_accuracy: 0.5735 - val_val_binary_crossentropy: 0.7165 - val_auc_8: 0.5960\n",
      "37\n",
      "Epoch 38/50\n",
      "2567/2567 - 21s - loss: 0.4804 - accuracy: 0.7631 - val_binary_crossentropy: 0.6324 - auc_8: 0.8471 - val_loss: 0.7449 - val_accuracy: 0.5760 - val_val_binary_crossentropy: 0.7144 - val_auc_8: 0.5929\n",
      "38\n",
      "Epoch 39/50\n",
      "2567/2567 - 21s - loss: 0.4801 - accuracy: 0.7632 - val_binary_crossentropy: 0.6323 - auc_8: 0.8473 - val_loss: 0.7976 - val_accuracy: 0.5683 - val_val_binary_crossentropy: 0.7234 - val_auc_8: 0.5917\n",
      "39\n",
      "Epoch 40/50\n",
      "2567/2567 - 21s - loss: 0.4799 - accuracy: 0.7635 - val_binary_crossentropy: 0.6322 - auc_8: 0.8475 - val_loss: 0.7543 - val_accuracy: 0.5747 - val_val_binary_crossentropy: 0.7160 - val_auc_8: 0.5857\n",
      "40\n",
      "Epoch 41/50\n",
      "2567/2567 - 21s - loss: 0.4795 - accuracy: 0.7635 - val_binary_crossentropy: 0.6321 - auc_8: 0.8477 - val_loss: 0.7590 - val_accuracy: 0.5734 - val_val_binary_crossentropy: 0.7176 - val_auc_8: 0.5903\n",
      "41\n",
      "Epoch 42/50\n",
      "2567/2567 - 21s - loss: 0.4792 - accuracy: 0.7638 - val_binary_crossentropy: 0.6320 - auc_8: 0.8480 - val_loss: 0.7928 - val_accuracy: 0.5690 - val_val_binary_crossentropy: 0.7212 - val_auc_8: 0.5860\n",
      "42\n",
      "Epoch 43/50\n",
      "2567/2567 - 21s - loss: 0.4791 - accuracy: 0.7638 - val_binary_crossentropy: 0.6319 - auc_8: 0.8480 - val_loss: 0.7717 - val_accuracy: 0.5745 - val_val_binary_crossentropy: 0.7173 - val_auc_8: 0.5940\n",
      "43\n",
      "Epoch 44/50\n",
      "2567/2567 - 21s - loss: 0.4787 - accuracy: 0.7641 - val_binary_crossentropy: 0.6318 - auc_8: 0.8484 - val_loss: 0.7959 - val_accuracy: 0.5699 - val_val_binary_crossentropy: 0.7224 - val_auc_8: 0.5897\n",
      "44\n",
      "Epoch 45/50\n",
      "2567/2567 - 21s - loss: 0.4785 - accuracy: 0.7641 - val_binary_crossentropy: 0.6318 - auc_8: 0.8484 - val_loss: 0.7739 - val_accuracy: 0.5719 - val_val_binary_crossentropy: 0.7190 - val_auc_8: 0.5934\n",
      "45\n",
      "Epoch 46/50\n",
      "2567/2567 - 21s - loss: 0.4783 - accuracy: 0.7645 - val_binary_crossentropy: 0.6317 - auc_8: 0.8486 - val_loss: 0.7437 - val_accuracy: 0.5791 - val_val_binary_crossentropy: 0.7139 - val_auc_8: 0.5895\n",
      "46\n",
      "Epoch 47/50\n",
      "2567/2567 - 21s - loss: 0.4779 - accuracy: 0.7646 - val_binary_crossentropy: 0.6316 - auc_8: 0.8489 - val_loss: 0.7377 - val_accuracy: 0.5724 - val_val_binary_crossentropy: 0.7139 - val_auc_8: 0.5849\n",
      "47\n",
      "Epoch 48/50\n",
      "2567/2567 - 21s - loss: 0.4776 - accuracy: 0.7649 - val_binary_crossentropy: 0.6314 - auc_8: 0.8491 - val_loss: 0.8027 - val_accuracy: 0.5654 - val_val_binary_crossentropy: 0.7227 - val_auc_8: 0.5867\n",
      "48\n",
      "Epoch 49/50\n",
      "2567/2567 - 21s - loss: 0.4775 - accuracy: 0.7648 - val_binary_crossentropy: 0.6314 - auc_8: 0.8492 - val_loss: 0.7664 - val_accuracy: 0.5702 - val_val_binary_crossentropy: 0.7156 - val_auc_8: 0.5857\n",
      "49\n",
      "Epoch 50/50\n",
      "2567/2567 - 21s - loss: 0.4772 - accuracy: 0.7651 - val_binary_crossentropy: 0.6313 - auc_8: 0.8494 - val_loss: 0.7546 - val_accuracy: 0.5728 - val_val_binary_crossentropy: 0.7158 - val_auc_8: 0.5905\n"
     ]
    }
   ],
   "source": [
    "Att_model = define_predictor()\n",
    "history = Att_model.fit(scaler.fit_transform(X_train),\n",
    "                        np.array(y_train),\n",
    "                        callbacks=get_callbacks(),\n",
    "                        verbose=2, \n",
    "#                        steps_per_epoch = 5, \n",
    "                        epochs=50, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        validation_data =  ( X_validate, y_validate ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 40s 2ms/step - loss: 0.4688 - accuracy: 0.7705 - val_binary_crossentropy: 0.6265 - auc_7: 0.8557\n"
     ]
    }
   ],
   "source": [
    "results = Att_model.evaluate(scaler.fit_transform(X_test),np.array(y_test), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Att_model.predict(scaler.fit_transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.902757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.401465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.593673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.902757\n",
       "1  0.704698\n",
       "2  0.401465\n",
       "3  0.593673\n",
       "4  0.599746"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf = pd.DataFrame(predictions)\n",
    "preddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "preddf['round'] = preddf.round(preddf[0])\n",
    "\n",
    "yresultsround = preddf['round'].tolist()\n",
    "yresultsround[2]\n",
    "y_test.shape\n",
    "y_test[2]\n",
    "\n",
    "ybinary = label_binarize(y_test, classes=[0,1])\n",
    "y_score = label_binarize(yresultsround, classes=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Power Used to Train and Test Replicated Model\n",
    "\n",
    "We used an AWS Deep Learning AMI. The AMIs provide machine learning practitioners and researchers with the infrastructure and tools to accelerate deep learning in the cloud, at any scale. You can quickly launch Amazon EC2 instances(servers) pre-installed with popular deep learning frameworks and interfaces such as TensorFlow, PyTorch, Apache MXNet, Chainer, Gluon, Horovod, and Keras to train sophisticated, custom AI models, experiment with new algorithms, or to learn new skills and techniques.  (https://aws.amazon.com/machine-learning/amis/)\n",
    "\n",
    "The server we utlized was an Amazon EC2 G3. The G3 instances are the latest generation of Amazon EC2 GPU graphics instances that deliver a powerful combination of CPU, host memory, and GPU capacity. G3 instances are ideal for graphics-intensive applications such as 3D visualizations, mid to high-end virtual workstations, virtual application software, 3D rendering, application streaming, video encoding, gaming, and other server-side graphics workloads.  \n",
    "\n",
    "Our specific instance  has 32 vCPUs based on custom 2.7 GHz Intel Xeon E5 2686 v4 processors and 244 GiB of DRAM host memory. Backed by2 NVIDIA Tesla M60 GPUs, with each GPU delivering up to 2,048 parallel processing cores and 8 GiB of GPU memory. (https://aws.amazon.com/ec2/instance-types/g3/)\n",
    "\n",
    "| Name | GPUs | vCPU | Memory (GiB) | GPU Memory (GiB) |\n",
    "|-|-|-|-|-|\n",
    "| g3.8xlarge | 2 | 32 | 244 | 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training, Evaluation and Testing Data** - Our ratio of train vs validation/test size was 70% to 30%. Of the 30% held back for validation and test, 50% was allocated to each. This generally follows using 80% or less to train with in order to avoid overfitting. \n",
    "\n",
    "In the paper, Baldi and team produced Receiver Operating Characteristic (ROC) curves to illustrate performance of the models, using the area under the ROC curve (AUC) as the measurement metric. This metric is directly related to classiciation accuracy and correlated to other metrics. Thus is a solid choice for comparison in this scenario. \n",
    "\n",
    "With AUC, larger values indicate higher classification accuracy, so our goal is to obtain higher values. From the paper, it appears they achieved an **AUC score of 0.88** for both the low-level feature set and the complete feature set when utilizing the dropout training algorithm (stochastically dropping neurons in the top hidden layer with 50% probablity during training). (We are most interested in the complete feature set as we used all the features in our comparative analysis). For our results, we were able to obtain an **AUC score of 0.8556 on the validation data** and an **AUC score of 0.8556 on prediction data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of ROC Curve: 0.8555619018456299\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "#Calculate the scores using the first model (replica)\n",
    " \n",
    "scores_1 = Att_model.predict(scaler.fit_transform(X_validate))\n",
    "\n",
    "fpr, tpr, thresholds = sk.metrics.roc_curve(y_validate, scores_1)\n",
    "\n",
    "#Display the ROC score and plot\n",
    "roc_auc = sk.metrics.roc_auc_score(y_validate, scores_1)\n",
    "\n",
    "print(\"AUC of ROC Curve:\", roc_auc)\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.title(\"ROC Curve\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of ROC Curve: 0.8556648679350376\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "#Calculate the scores using the first model (replica)\n",
    "scores_1 = Att_model.predict(scaler.fit_transform(X_test))\n",
    "\n",
    "fpr, tpr, thresholds = sk.metrics.roc_curve(y_test, scores_1)\n",
    "\n",
    "#Display the ROC score and plot\n",
    "roc_auc = sk.metrics.roc_auc_score(y_test, scores_1)\n",
    "\n",
    "print(\"AUC of ROC Curve:\", roc_auc)\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.title(\"ROC Curve\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now vs Then: Best Practices and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 1: Explore New Python Packages and Other Software\n",
    "Today, there are newer packages that require a lot less effort and are more efficient than **TensorFlow** and **Keras**. TensorFlow is an end to end open-source library used for machine learning and neural networks that are simple to understand, but does not allow for much flexibility as some of the newer APIs. \n",
    "\n",
    "PyTorch is becoming one of the more popular packages out there. PyTorch is a Python-based scientific computing package that combines the flexibility of NumPy arrays with the power to use GPUs. (PyTorch tensors are similar to NumPy arrays.) It is designed to provide maximum flexibility and speed. PyTouch is model to be similar to Pandas, so the learning curve is much more gradual.\n",
    "\n",
    "\n",
    "\n",
    "### Activation Functions: We would explore using the newer activation functions of ReLU, Softwmax vs tanh or Sigmoid.\n",
    "\n",
    "\n",
    "**Softmax activation** – \"This function converts numbers to probabilities that sum up to one. It outputs a vector that denotes the probability distributions of a list of potenmtial outcomes\"\n",
    "![title](img/Softmax.png) \n",
    "\n",
    "Reference: https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d#:~:text=Specifically%20trying%20out%20neural%20networks,a%20list%20of%20potential%20outcomes.\n",
    "\n",
    "**Sigmoid activation** - \"Sigmoid activation functions use a second non-linearity for large inputs. Being in the range from zero and one, sigmoid activations can be interpreted as probabilities. If a range from -1 to 1 is desired, the sigmoid can be scaled and shifted to yield the hyperbolic tangent activation function\"\n",
    "![title](img/Sigmoid.png) \n",
    "\n",
    "Reference: https://en.wikipedia.org/wiki/Activation_function#:~:text=Sigmoid%20activation%20functions%20use%20a,the%20hyperbolic%20tangent%20activation%20function%3A%20.\n",
    "\n",
    "**tanh** - \"tanh is similar to a logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped). The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\"\n",
    "![title](img/tanh.png) \n",
    "\n",
    "Reference: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6#:~:text=tanh%20is%20also%20like%20logistic,sigmoidal%20(s%20%2D%20shaped).&text=The%20advantage%20is%20that%20the,zero%20in%20the%20tanh%20graph.\n",
    "\n",
    "\n",
    "**ReLU activation** – Rectified Linear Unit activation function, is a piecewise linear function that outputs the input directly if positive (otherwise, output 0). Default activation, easy train, better performance, activation is threshold at 0, can accelerate SGD, implemented by simply thresholding matrix of activations at 0, can be fragile where weights could update in a way for neuron to not activate again (Leaky ReLU attempts to fix dying problem)\n",
    "![title](img/ReLU_vs_Sigmoid.png) \n",
    "\n",
    "\n",
    "### Sampling\n",
    "- Training and Testing using different splits of the data. Baldi et al used 99% of the data for training and a 1% for testing.  We recommend using an 80% train and 20% test setbut other options such as 70/30 should also be explored.\n",
    "\n",
    "### ADAM Optimizer\n",
    "- \"Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iteratively based on the training data.\"  We leveraged the Adam optimizer for our model but clearly given our accuracyt scores, more refinement is required to obtian the most optimal parameter settings.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20an%20optimization%20algorithm,iterative%20based%20in%20training%20data.&text=The%20algorithm%20is%20called%20Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXJxtwBWIhjG"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "While we were able to leverage the use of different hyper-parameters and functions within Tensorflow and Keras, we were not able to achieve the same ROC scores per Baldi et al in their original paper. It is also evident that neural networks provides extensive functionality and power to build models of such complexity with relative ease and reduced coding. The key learning for us was the importance of **tuning** the hyper-parameters and therefore understanding their impact on model performance and execution. Learning rate and the time spent training had the highest impact. Most of the other parameters adjust how learning rate behaved through out the training process. We found that a large learning rate was needed to get model to increase accuracy at an acceptable rate.  We would increase the number of epochs to give the model more time find the minimum if we had more time and resources available to match Baldi’s research."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "overfit_and_underfit.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
